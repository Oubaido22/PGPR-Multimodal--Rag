#!/usr/bin/env python3
"""
Script pour vÃ©rifier la mÃ©moire systÃ¨me et le statut d'Ollama
"""

import psutil
import requests
import subprocess
import sys

def check_system_memory():
    """VÃ©rifie la mÃ©moire systÃ¨me disponible"""
    print("ğŸ” VÃ©rification de la mÃ©moire systÃ¨me...")
    
    # MÃ©moire totale
    total_memory = psutil.virtual_memory().total / (1024**3)  # GB
    available_memory = psutil.virtual_memory().available / (1024**3)  # GB
    used_memory = psutil.virtual_memory().used / (1024**3)  # GB
    
    print(f"ğŸ“Š MÃ©moire totale: {total_memory:.1f} GB")
    print(f"ğŸ“Š MÃ©moire utilisÃ©e: {used_memory:.1f} GB")
    print(f"ğŸ“Š MÃ©moire disponible: {available_memory:.1f} GB")
    
    # Recommandations
    if available_memory < 5.0:
        print("âš ï¸  MÃ©moire disponible faible (< 5 GB)")
        print("ğŸ’¡ Recommandations:")
        print("   - Fermez d'autres applications")
        print("   - Utilisez les paramÃ¨tres optimisÃ©s (dÃ©jÃ  appliquÃ©s)")
        print("   - ConsidÃ©rez utiliser un modÃ¨le plus petit")
    elif available_memory < 8.0:
        print("âœ… MÃ©moire disponible correcte (5-8 GB)")
        print("ğŸ’¡ Les paramÃ¨tres optimisÃ©s devraient fonctionner")
    else:
        print("âœ… MÃ©moire disponible excellente (> 8 GB)")
        print("ğŸ’¡ Vous pourriez augmenter les paramÃ¨tres si nÃ©cessaire")
    
    return available_memory

def check_ollama_status():
    """VÃ©rifie le statut d'Ollama"""
    print("\nğŸ” VÃ©rification d'Ollama...")
    
    try:
        # VÃ©rifier si Ollama est en cours d'exÃ©cution
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            print("âœ… Ollama est en cours d'exÃ©cution")
            
            # VÃ©rifier les modÃ¨les disponibles
            models = response.json().get("models", [])
            if models:
                print("ğŸ“‹ ModÃ¨les disponibles:")
                for model in models:
                    name = model.get("name", "Unknown")
                    size = model.get("size", 0) / (1024**3)  # GB
                    print(f"   - {name}: {size:.1f} GB")
            else:
                print("âš ï¸  Aucun modÃ¨le trouvÃ©")
                return False
        else:
            print("âŒ Ollama ne rÃ©pond pas correctement")
            return False
            
    except requests.exceptions.ConnectionError:
        print("âŒ Ollama n'est pas en cours d'exÃ©cution")
        print("ğŸ’¡ DÃ©marrez Ollama avec: ollama serve")
        return False
    except Exception as e:
        print(f"âŒ Erreur lors de la vÃ©rification d'Ollama: {e}")
        return False
    
    return True

def check_llama_model():
    """VÃ©rifie spÃ©cifiquement le modÃ¨le llama3.1"""
    print("\nğŸ” VÃ©rification du modÃ¨le llama3.1...")
    
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        models = response.json().get("models", [])
        
        llama_models = [model for model in models if "llama3.1" in model.get("name", "")]
        
        if llama_models:
            model = llama_models[0]
            name = model.get("name", "Unknown")
            size = model.get("size", 0) / (1024**3)  # GB
            print(f"âœ… ModÃ¨le trouvÃ©: {name}")
            print(f"ğŸ“Š Taille: {size:.1f} GB")
            
            if size > 4.0:
                print("âš ï¸  Le modÃ¨le est assez volumineux")
                print("ğŸ’¡ Les paramÃ¨tres optimisÃ©s sont nÃ©cessaires")
            else:
                print("âœ… Taille du modÃ¨le acceptable")
            
            return True
        else:
            print("âŒ ModÃ¨le llama3.1 non trouvÃ©")
            print("ğŸ’¡ Installez-le avec: ollama pull llama3.1")
            return False
            
    except Exception as e:
        print(f"âŒ Erreur lors de la vÃ©rification du modÃ¨le: {e}")
        return False

def suggest_optimizations():
    """SuggÃ¨re des optimisations"""
    print("\nğŸ’¡ Optimisations appliquÃ©es:")
    print("âœ… Context window rÃ©duit: 2048 â†’ 1024")
    print("âœ… Threads rÃ©duits: 4 â†’ 2")
    print("âœ… GPU dÃ©sactivÃ©: num_gpu = 0")
    print("âœ… RÃ©ponse limitÃ©e: num_predict = 256")
    print("âœ… Mode low VRAM activÃ©")
    
    print("\nğŸš€ Si le problÃ¨me persiste:")
    print("1. Fermez d'autres applications")
    print("2. RedÃ©marrez Ollama: ollama serve")
    print("3. Utilisez un modÃ¨le plus petit: ollama pull llama3.1:8b")
    print("4. Augmentez la mÃ©moire virtuelle si possible")

def main():
    """Fonction principale"""
    print("ğŸ§¬ VÃ©rificateur de mÃ©moire et Ollama")
    print("=" * 50)
    
    # VÃ©rifier la mÃ©moire
    available_memory = check_system_memory()
    
    # VÃ©rifier Ollama
    ollama_ok = check_ollama_status()
    
    # VÃ©rifier le modÃ¨le
    model_ok = check_llama_model()
    
    # Suggestions
    suggest_optimizations()
    
    print("\nğŸ“‹ RÃ©sumÃ©:")
    print(f"   MÃ©moire disponible: {available_memory:.1f} GB")
    print(f"   Ollama: {'âœ…' if ollama_ok else 'âŒ'}")
    print(f"   ModÃ¨le llama3.1: {'âœ…' if model_ok else 'âŒ'}")
    
    if ollama_ok and model_ok and available_memory > 4.0:
        print("\nğŸ‰ SystÃ¨me prÃªt! Vous devriez pouvoir utiliser l'interface.")
    else:
        print("\nâš ï¸  ProblÃ¨mes dÃ©tectÃ©s. VÃ©rifiez les recommandations ci-dessus.")

if __name__ == "__main__":
    main()
